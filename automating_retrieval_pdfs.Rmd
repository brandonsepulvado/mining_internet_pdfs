---
title: "Automating PDF Retrieval from Websites"
author: "Brandon Sepulvado"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(rvest)
library(dplyr)
library(glue)
library(pdftools)
library(tesseract)
library(stringr)
library(tidytext)
```


# Problem

I study the emergence of new fields and categorization practices within the biomedical sciences, and bioethics is a key domain where academics and practitioners debate issues affecting patients, policies, and the organziation of societal risks. France has an interesting organization, called the __Comité Consultatif National d'Éthique__ (henceforth, CCNE), that provides ethical guidance for various policies. The [CCNE](http://www.ccne-ethique.fr/en) may help the President of France, the presidents of the National Assembly and Senate, higher education insitutions, various public organziations, and certain foundations. The CCNE was established in 1983 by the government and regularly releases opinions. 

I want to download all of the opinions released to date (127 in English, 128 in French). Given the relatively small number, this task could be accomplished manually, but I want to replicate it for several international organization. Thus, I will use R to automate it. 

# Getting the PDFs

I will obtain the opinion PDFs in two basic steps: get the file links and download the files. 

There are multiple ways to accomplish these tasks, but I will demonstrate two. 

## Obtain File Links with rvest

The [rvest](https://github.com/hadley/rvest) package is probably the best documented of the potential methods and was created by Hadley Wickham. It basically serves to facilitate work with the `xml2` and `httr` packages, which can be rather complex and difficult to use. There are multiple good tutorials for rvest, such as [this](https://www.datacamp.com/community/tutorials/r-web-scraping-rvest) one from DataCamp and [thus](https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/) from RStudio. 

### CSS SelectorGadget

Before we can extract the links to the opinion files, we need to know where exactly in the underlying code for the webpage is housed the file paths. I will use the CSS selector. Luckily, one does not have to be a CSS/HTML expert (I certainly am not); [selectorgadget](https://selectorgadget.com/) provides a wonderful bookmarklet and Chrome extension (I use the latter).

Insert gif of selection process here. 

The main/first page for the CCNE opinions is this: http://www.ccne-ethique.fr/en/type_publication/avis. When you navigate to this link, you will see a list of the several latest opinions, and several "unofficial" opinions, which are not part of their formal opinion series. The latter will not have a number (in the No. field). You cannot download an opinion on this page; rather, you must click on the title (titre) of an opinion. If you click on the latest opinion, number 127, you will be directed to a page with a summary of the opinion and a button to download the PDF. The summaries are not consistently this long or detailed, so I want to actual files. 

The process to identify the CSS selector is quite simple. Click on the Chrome extension and then on the "Download Document" button. You will see that there are many other parts of the page that are highlighted in yellow, which is a problem. As such, I click on the CCNE link at the top left of the page to tell the selectorgadget that I do not want it; now, the only item highlighted is the button. The selectorgadget indicates that the CSS selector is _"#main-content a"_. Note: I repeated this process on multiple other opinion pages, and every repetition gave the same CSS selector.

### Using CSS Selector with rvest

Now that we know which part of the webpage we want, let's use rvest to get it. In the following code chunk, I load the package and set the webpage to scrape. 

```{r load_setpage}
# load package
library(rvest)

# set page
webpage <- read_html("http://www.ccne-ethique.fr/en/publications/ccnes-opinion-ndeg-127-migrants-health-and-ethical-imperatives")
```

Now, let's extract the information we want.

```{r}
pdf_link_incorrect <- webpage %>% 
  html_node("#main-content a") %>% 
  html_text()
pdf_link_incorrect
```

Oh no, something is wrong! The output says "Home". This obviously is not what we want, and the reason has to do with the (incorrect) CSS selector. Instead of using the selectorgadget, I will now look at the webpage code to see if I can get a better idea of the correct selector. 

Right click on the "download document" button, and choose inspect. You should then see a pane with the relevant code selected; see the image below. 

![Figure 1: Code inspection](inspect_element.png)


You can see that the selected section falls within a div section with a panel-content class and that the section immediately containing the pdf link has an a tag. The actual link is an href attribute. I, in this next chunk of code, use the new information to extract the link. 

```{r extract_link}
pdf_link <- webpage %>% 
  html_node("div.pane-content a") %>% 
  html_attr("href") 
```

Voilà! We now have the link, but how to get the pdf text into R? There are two options. I will show both, but I will only use the second.

First, you might want to download the file onto your hard drive (or server). I will not execute the following code.

```{r download_pdf, eval = FALSE}
download.file(pdf_link,
              "./opinion.pdf")
```
The first piece of required input is the pdf link (`pdf_link`), and the second is the file destination on your local drive or server. 

However, I do not want to download the actual files; I just want the text. There are multiple packages that will read text from pdf files, but I will use the pdftools package. By using the `pdf_text` function, we can go directly from the `pdf_link` object to a new object containing the opinion text, bypassing the need to save the pdf file.

```{r read_opinion_text}
opinion <- pdf_text(pdf_link)
```

This works! I will postpone a detailed examination of the `opinion` object for later. However, there is a key limitation to this approach: we need the links to 127 (English) documents, so let's focus on getting the rest of them. The ideal way to do so would be to set up a crawler to go through the site for us, but that's a bit advanced for this workshop. For now, we will strategically going through the relevant sections of the site and select only what we want.   

## Getting ALL the Opinions

To recap, the previous approach will largely work for downloading all opinions; we just need to expand it a bit. From each *opinion's* page, we selected the *pdf* link that we then read the PDF's text into an R object. The problem now is twofold: (1) collect all the page urls for each opinion and (2) connect these to the previous workflow. 

The major problem with this website is that it does not list all the opinions on a single page and that the structure of the first and all other publication pages' url differs. The general set of tasks is as follows:

    1. Gather all the CCNE opinion links for a single page of publications section
    2. Repeat for every page of publications section
    3. Combine information into a single object

### Main (Publication) Pages

In this section, I want to explore each main page of the publications section and to get the link to the page with information about each opinion and its pdf link. 

```{r links_from_single_mainpage}
# set page
webpage_pubs <- read_html("http://www.ccne-ethique.fr/en/publications")

# get links for each publication page associated with these titles
pub_page_links <- webpage_pubs %>% 
  html_nodes("td.views-field.views-field-title a") %>% 
  html_attr("href")

# print result
pub_page_links
```

As you can see, `pub_page_links` contains a link from the landing publications page (`webpage_pubs`), but it is relative: we cannot enter it into a browser and be taken to the publication's page. Rather, we need to "complete" the url with that of the main site. The following code chunk uses the glue package to accomplish this task. 

```{r complete_pub_page_links}
# complete links (with initial site info)
pub_page_links <- glue("http://www.ccne-ethique.fr/{pub_page_links}")
```

We now have a character vector of ten complete urls to each of the CCNE opinions that are listed on the main publications landing page. I write, in the next bit of code, a function that takes these links and returns the links to each page's opinion pdf. 
```{r get_pdf_function_1}
# function to get pdf links from each page (row) above
get_pdf_link <- function(pub_page_link){
  pdf_link <- read_html(pub_page_link) %>% 
    html_node("div.pane-content a") %>% 
    html_attr("href")
}

# apply to all links pub_page_links
pdf_links_10 <- lapply(pub_page_links, get_pdf_link) %>% 
  unlist() %>% 
  as_tibble() %>% 
  rename(pdf_link = value)

# print result
pdf_links_10
```
There are a few things to note here. The function take *a single publication page* and returns its pdf link, and, after writing that, I use `lapply()` to apply this function to each url within `pub_page_link`, after which I collapse the resultant list into a tibble. In many cases, you can use `bind_rows()` from the dplyr package after `lapply()`, but, because `bind_rows()` relies on column names, it does not work in this instance; this issue is why pipe into the `unlist()`-`as_tibble()`-`rename()` chain. 

However, there is a big issue: the fourth row does not contain a pdf link. The entry is for a footnote on that page, which can be resolved with effort (that is not easily generalizable to other problems), but I will, for the time being, simply rewrite the function to replace this value with an NA. I use the rule that each url should contain "pdf". 

```{r get_pdf_function_2}

# function to get pdf links from each page (row) above
get_pdf_link <- function(pub_page_link){
  pdf_link <- read_html(pub_page_link) %>% 
    html_node("div.pane-content a") %>% 
    html_attr("href") 
  
  if(!stringr::str_detect(pdf_link, "\\.pdf")) {
    pdf_link <- NA
  }
  
  return(pdf_link)
}

# apply to all links pub_page_links
pdf_links_10 <- lapply(pub_page_links, get_pdf_link) %>% 
  unlist() %>% 
  as_tibble() %>% 
  rename(pdf_link = value)

# print result
pdf_links_10
```

### All Publication Pages

In the last section, we took a single "main" page of the CCNE website's publication section, where 10 opinions are listed and where we can find the url for each of those opinions' webpage. We obtained a part of each of the 10 urls and then completed. Next, we wrote a function to extract the pdf link contained within each of the publication webpages. The task now is to repeat this process for each "main" publication page. 

![Figure 2: Getting last page](page_numbers.png)

Using the CSS selector identified in Figure 2, I now extact the html attribute for the last page, just to make inspection easier. 
```{r}
# get total number of pages
webpage_pubs %>% 
  html_nodes("ul.pager li.last a") %>% 
  html_attr("href")
```
This part of the url indicates 13 as the page number, but, if you compare carefully this url with the initial page ("http://www.ccne-ethique.fr/en/publications"), you should notice that they have distinct structures. The page number for page two begins with one, and the structure for the remaining pages remains the same. We thus have a total of 14 "main" publication pages.

The next code chunk creates `all_main_pages`, which creates character vector of all main page urls. 
```{r}

### get list of all publication (main) pages

# first page has unique url
first_page <- "http://www.ccne-ethique.fr/en/publications"

# create numeric vector to input into url vector
numbers <- seq(from = 1, to = 13, by = 1)

# rest (2-13): common format
other_pages <- glue("http://www.ccne-ethique.fr/en/publications?page={numbers}")

# combine first_page and other_pages and make tibble
all_main_pages <- c(first_page, other_pages) #%>% 
  #as_tibble() %>% 
  #rename(main_pagelink = value)

# print result
all_main_pages

```


In a previous section, I extracted all the publication pages for opinions listed on the main "main" page. Now, I want to use the list of all "main" pages as input and receive as output all of the publication pages. 

This section includes the `get_pub_pages` function, which is very simple and requires the url to one of the main pages. The following line of code applies this function to `all_main_pages`. 
```{r}
# create function to get each publication's link from each "main page"
get_pub_pages <- function(main_page){
  read_html(main_page) %>% 
  html_nodes("td.views-field.views-field-title a") %>% 
  html_attr("href")
}

# apply function to all main pages
all_pub_pages <- lapply(all_main_pages, get_pub_pages)

# check out object
class(all_pub_pages)
length(all_pub_pages)
```
Applying the function to all main pages returns a list of 14 elements because there are 14 main pages. 


```{r}
all_pub_pages[[1]]
```

However, examining the first element of `all_pub_pages` shows that they are again not urls that will permit R to navigate to the pages. Thus, I need to complete them.
```{r}

# complete url NOT USING GLUE DUE TO POTENTIAL VECTORIZATION ISSUE???
#all_pub_pages <- lapply(all_pub_pages, function(x){
#  as_tibble(x) %>% 
#    rename(partial_pub_page_link = value) %>% 
#    mutate(full_pub_page_link = #glue("http://www.ccne-ethique.fr{partial_pub_page_link}"))})

# complete url
all_pub_pages <- lapply(all_pub_pages, function(x){
  as_tibble(x) %>% 
    rename(partial_pub_page_link = value) %>% 
    mutate(full_pub_page_link =
             paste0("http://www.ccne-ethique.fr", partial_pub_page_link))})

# collpase into tibble
all_pub_pages <- bind_rows(all_pub_pages)

# check out result
class(all_pub_pages)
dim(all_pub_pages)
names(all_pub_pages)
head(all_pub_pages, 10)
```
Using `lapply()`, I create a new variable, named `full_pub_page_link`, within each list element containing the full url, and I then use bind rows to collapse the list into a tibble. I am able to do so now because I named both columns (i.e., `partial_pub_page_link` and `full_pub_page_link`). I commented out a section of code that warrants mention. You may use the `glue()` command to accomlish the same work as `paste0()`, but vectorization creates a poential issue. Even though the results seemed fine with `glue()`, I sided with `paste0()` to be safe.

## Returning to Text Import

Now that we have completed the three tasks (i.e., gather all the CCNE opinion links for a single page of publications section, repeat for every page of publications section, and combine information into a single object), we need to get the text from the CCNE opinion pdfs. I earlier illustrated two ways of doing this: you can, using the pdftools package, download the files or read the text of each directly into R. I will opt for the latter.

But, before doing that, let's use the `get_pdf_link()` function from earlier to get every pdf link based upon `all_pub_pages$full_pub_page_link`.
```{r get_all_pdf_links}
# get pdf links
all_pdf_links <- lapply(all_pub_pages$full_pub_page_link, get_pdf_link) 

# collapse
all_pdf_links <- all_pdf_links %>% 
  unlist() %>% 
  as_tibble() %>% 
  rename(pdf_link = value)
```


This code saves the pdf links, so I do not have to access the site from now on every time I would like to reference the links. 
```{r}
# save so don't have to access site every time
saveRDS(all_pdf_links, file = "all_pdf_links.rds")
```


```{r eval=FALSE, include=FALSE}
# ran when lapply didn't work
list <- vector(mode = "list", length = nrow(all_pub_pages))
for (i in 1:20){
  list[[i]] <- get_pdf_link(all_pub_pages$full_pub_page_link[i])
}
```


We have now arrived at the big moment: extracting the text from all pdfs. Before going so, it is necessary to remove the two NAs because `pdf_text()` will attempt to treat them as urls.

```{r, error=TRUE}
all_pdf_links_nomiss <- all_pdf_links %>% 
  filter(!is.na(pdf_link))

opinion_texts <- lapply(all_pdf_links_nomiss$pdf_link, pdf_text) 
  
```
Oh no! We get a ton of errors that do not provide much direction about where exactly the problem is located. 

To help solve this problem, I'm going to go with a computationally less efficient though perhaps more straightforward manner of identifying problematic areas/files. First, I create a variable that is simply the case/row id; second, I write a for loop to report that a row is successfully handled, which will at a minimum let us know where the problem originates.
```{r}
all_pdf_links_nomiss <- all_pdf_links_nomiss %>% 
  mutate(row_id = 1:nrow(all_pdf_links_nomiss))

test_text <- vector(mode = "list", length = nrow(all_pdf_links_nomiss))
for (i in 1:nrow(all_pdf_links_nomiss)){
  test_text[[1]] <- pdf_text(all_pdf_links_nomiss$pdf_link[i])
  message(paste0("Row ", all_pdf_links_nomiss$row_id[i]))
}
```
You can see where the errors from earlier begin, but this does not mean that everything else went fine. The following code identifies how many list elements are not empty. If the rows that did not exhibit the above errors are fine, then they should have text, meaning each element should have a length > 0. 
```{r test_element_length}
# how many have length > 0
sum(sapply(test_text, function(x){length(x) > 0}))
```
Because there is only one element that has a length greater than zero, there are major problems, and, given the time limit today, I will not go into detail about the causes of these issues (though feel free to contact me if you would like to discuss it in further detail). For now, we will move to another solution. 


### Tesseract

The `pdf_text()` function has problems with several opinions, such as opinion 69 (on the website, not `all_pdf_links` row 69). We get text, but it is unrecognizable as a natural language. If you would like to see an example, you may run the following code; I do not do so here. 
```{r pdf_text_prob_ex, eval=FALSE}
pdf_text("http://www.ccne-ethique.fr/sites/default/files/publications/avis069en.pdf")
```

Another option, which is generally less preferable than `pdf_text()` for reasons you will see, is the tesseract package.[^1] The workflow is decidedly less direct. With this package, you must first convert a pdf to an image file; the pdftools package's `pdf_convert()` can help with this. You will notice that doing so creates an image in the working directory for each page of the pdf; this is a clear limitation to this approach, as the previous one did not require you to download new files. However, in cases such as the present where there are major errors prohibiting task completion, the tradeoff seems to be justified. When automating this process, we will be sure to delete the image files after OCRing the set for a single CCNE opinion. 

[^1]: [Here](https://cran.r-project.org/web/packages/tesseract/vignettes/intro.html) is a good introduction/vignette.

```{r tesseract_first_attempt}
# convert to image (pdftools)
pngfile <- pdftools::pdf_convert('http://www.ccne-ethique.fr/sites/default/files/publications/avis069en.pdf', dpi = 600)

# convert to text
text <- tesseract::ocr(pngfile)
#cat(text)

# remove pnd files
sapply(pngfile, file.remove)
```
We again get errors, but, after inspecting the images, nothing seems to be off in any detrimental sense. Further, the text seems to be decent. The `sapply()` function tells us that all of the png files have been deleted from the working directory. 


We now can replace the `pdf_text()` function that we used in the `lapply()` to get `opinino_texts` with the tesseract approach, but a new function is needed due to the indirect conversion, ocr, and deletion process we undertook immediately above. 
```{r get_text_try2, eval=FALSE}
# new function
get_pdf_text <- function(x) {
  pngfile <- pdftools::pdf_convert(x, dpi = 600)

  # convert to text
  return(tesseract::ocr(pngfile))

  # remove pnd files
  invisible(sapply(pngfile, file.remove))
}

# get texts
opinion_texts_tes <- lapply(all_pdf_links_nomiss$pdf_link, get_pdf_text) 

```

Unfortunately, we get a bad error message: the R session aborts. This issue turns out to be well *identified*, though poorly *documented* in tesseract (check out the github page), but there is no clear solution. There are likely inconsistencies in new and legacy dictionaries that the `ocr()` function callss. It is possible to dig down into the code and tell tesseract to ignore the legacy dictionary, but that is far beyond the scope of this example. 



## Tidying the Output(s)

Obviously, this outcome is less than ideal; I would have preferred to show you a workflow that runs without issue. I could have shown you a successful example with the altered tesseract code, but I chose to leave the various errors (e.g., selectorgadet problem, various ocr errors with pdftools and tesseract) because these are ones that you would likely encounter if you undertake a task similar to the one at hand. If you do run into these or similar problems, feel free to come to me; the University also has multiple centers that offer help: the [Center for Social Research](https://csr.nd.edu/), the [Center for Research Computing](crc.nd.edu), and the [Navari Family Center for Digital Scholarship](http://cds.library.nd.edu/). Know that these issues are not insurmountable. 

Before I conclude however, I want to return to getting the output structured in a way easily applicable to sociological analyses. Let's return to the `opinion` object, which contains the text of the 127th opinion.

```{r org_results_1}
class(opinion)
length(opinion)
```
This code indicates that it is a character vector of length 26. `opinion` is structured such that each element is a page from the pdf, and "\n" indates a line break. To prepare for this example, let's also import opinino 126, which is the second row in  `all_pdf_links`. This document has 75 pages. 
```{r org_results_2}
opinion_126 <- pdf_text(as.character(all_pdf_links[2,1]))
class(opinion_126)
length(opinion_126)
```

I ultimately want a data set in which there is one word per row and with variables indicating the document, the page number, and the line. The package upon which we will most heavily rely is [tidytext](https://www.tidytextmining.com/).[^2]

[^2]: Also, check out Julia Silge's [blog](https://juliasilge.com/).

```{r org_results_3}
# create document variable
opinion <- opinion %>% 
  as_tibble() %>% 
  rename(page_content = value) %>% 
  mutate(opinion = "127")
opinion_126 <- opinion_126 %>% 
  as_tibble() %>% 
  rename(page_content = value) %>% 
  mutate(opinion = "126")

# combine opinion documents
opinion_all <- bind_rows(opinion, opinion_126)
```


Now, let's add the page number variable. The following code takes the data and uses `groupy_by()` to partition the rows into exclusive sets based upon the CCNE opinion in which they are found. There will be two groups in our case because we only have two opinions, but, if for example we had all of them, there would be almost 130 groups. `mutate()` then adds a variable called `page_num` that corresponds to the page number *within each group/opinion*. 
```{r page_numbers}
# add page number within each opinion
opinion_all <- opinion_all %>% 
  group_by(opinion) %>% 
  mutate(page_num = row_number())

# check out new variable
opinion_all
```


Now for the line numbers: Recall that lines are indicated by "\n". Creating a variable for the page number was quite straightforward because the pages corresponded to rows, but now that is not the case. We will use the tidytext package (specifically, the `unnest_tokens()` function) to split the `page_content` cells into multiple rows and then create a new variable `line_num`. Note that, if there were variables for each row when rows corresponded to a page, their contents would be copied to each new row that comes from the respective previous row.
```{r line_numbers}
# add page number within each opinion
opinion_all <- opinion_all %>% 
  unnest_tokens(line_content, page_content, # line_content is a new variable
                token = stringr::str_split, pattern = "\n") %>%  
  group_by(opinion, page_num) %>% 
  mutate(line_num = row_number())

# check out new variable
opinion_all
```

Finally, let's unnest once more in order to get one word per row. 



Finally, let's unnest this result into an object where rows are single words. 
```{r final_unnest}
# unnest to word rows
opinion_all <- opinion_all %>% 
  unnest_tokens(word, line_content)

# print result
opinion_all
```
Notice that the variables from the previous iteration where the row unit was the line have been transferred to the respective words. 

Now, what if one uses the tesseract option? There should be no difference in the tidying process because that process saves one .png per page and then returns a character vector in which each element is a page. Lines are again separated by "\n". 
